{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems Tutorial\n",
    "In this tutorial, we'll provide a simple walkthrough of how to use Snorkel to build a recommender system.\n",
    "We consider a setting similar to the [Netflix challenge](https://www.kaggle.com/netflix-inc/netflix-prize-data), but with books instead of movies.\n",
    "We have a set of users and books, and for each user we know the set of books they have interacted with (read or marked as to-read).\n",
    "We don't have the user's numerical ratings for the books they read, except in a small number of cases.\n",
    "We also have some text reviews written by users.\n",
    "\n",
    "Our goal is to build a recommender system by training a classifier to predict whether a user will read and like any given book.\n",
    "We'll train our model over a user-book pair to predict a `rating` (a `rating` of 1 means the user will read and like the book).\n",
    "To simplify inference, we'll represent a user by the set of books they interacted with (rather than learning a specific representation for each user).\n",
    "Once we have this model trained, we can use it to recommend books to a user when they visit the site.\n",
    "For example, we can just predict the rating for the user paired with a book for a few thousand likely books, then pick the books with the top ten predicted ratings.\n",
    "\n",
    "Of course, there are many other ways to approach this problem.\n",
    "The field of [recommender systems](https://en.wikipedia.org/wiki/Recommender_system) is a very well studied area with a wide variety of settings and approaches, and we just focus on one of them.\n",
    "\n",
    "We will use the [Goodreads](https://sites.google.com/eng.ucsd.edu/ucsdbookgraph/home) dataset, from\n",
    "\"Item Recommendation on Monotonic Behavior Chains\", RecSys'18 (Mengting Wan, Julian McAuley), and \"Fine-Grained Spoiler Detection from Large-Scale Review Corpora\", ACL'19 (Mengting Wan, Rishabh Misra, Ndapa Nakashole, Julian McAuley).\n",
    "In this dataset, we have user interactions and reviews for Young Adult novels from the Goodreads website, along with metadata (like `title` and `authors`) for the novels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "md-exclude"
    ]
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "if os.path.basename(os.getcwd()) == \"snorkel-tutorials\":\n",
    "    os.chdir(\"recsys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by running the `download_and_process_data` function.\n",
    "The function returns the `df_train`, `df_test`, `df_dev`, `df_valid` dataframes, which correspond to our training, test, development, and validation sets.\n",
    "Each of those dataframes has the following fields:\n",
    "* `user_idx`: A unique identifier for a user.\n",
    "* `book_idx`: A unique identifier for a book that is being rated by the user.\n",
    "* `book_idxs`: The set of books that the user has interacted with (read or planned to read).\n",
    "* `review_text`: Optional text review written by the user for the book.\n",
    "* `rating`: Either `0` (which means the user did not read or did not like the book) or `1` (which means the user read and liked the book). The `rating` field is missing for `df_train`.\n",
    "Our objective is to predict whether a given user (represented by the set of book_idxs the user has interacted with) will read and like any given book.\n",
    "That is, we want to train a model that takes a set of `book_idxs` (the user) and a single `book_idx` (the book to rate) and predicts the `rating`.\n",
    "\n",
    "In addition, `download_and_process_data` also returns the `df_books` dataframe, which contains one row per book, along with metadata for that book (such as `title` and `first_author`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "md-exclude-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Downloading raw data\n",
      "INFO:root:Processing book data\n",
      "INFO:root:Processing interaction data\n",
      "/home/ubuntu/snorkel-tutorials/recsys/utils.py:223: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_interactions_nz[\"rating_4_5\"] = df_interactions_nz.rating.map(ratings_map)\n",
      "INFO:root:Processing review data\n",
      "INFO:root:Joining interaction data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>book_id</th>\n",
       "      <th>country_code</th>\n",
       "      <th>description</th>\n",
       "      <th>is_ebook</th>\n",
       "      <th>language_code</th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>similar_books</th>\n",
       "      <th>text_reviews_count</th>\n",
       "      <th>title</th>\n",
       "      <th>first_author</th>\n",
       "      <th>book_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[293603]</td>\n",
       "      <td>4.35</td>\n",
       "      <td>10099492</td>\n",
       "      <td>US</td>\n",
       "      <td>It all comes down to this.\\nVlad's running out...</td>\n",
       "      <td>True</td>\n",
       "      <td>eng</td>\n",
       "      <td>152</td>\n",
       "      <td>[25861113, 7430195, 18765937, 6120544, 3247550...</td>\n",
       "      <td>9</td>\n",
       "      <td>Twelfth Grade Kills (The Chronicles of Vladimi...</td>\n",
       "      <td>293603</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[4018722]</td>\n",
       "      <td>3.71</td>\n",
       "      <td>22642971</td>\n",
       "      <td>US</td>\n",
       "      <td>The future world is at peace.\\nElla Shepherd h...</td>\n",
       "      <td>True</td>\n",
       "      <td>eng</td>\n",
       "      <td>1525</td>\n",
       "      <td>[20499652, 17934493, 13518102, 16210411, 17149...</td>\n",
       "      <td>428</td>\n",
       "      <td>The Body Electric</td>\n",
       "      <td>4018722</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[6537142]</td>\n",
       "      <td>3.89</td>\n",
       "      <td>31556136</td>\n",
       "      <td>US</td>\n",
       "      <td>A gorgeously written and deeply felt literary ...</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>109</td>\n",
       "      <td>[]</td>\n",
       "      <td>45</td>\n",
       "      <td>Like Water</td>\n",
       "      <td>6537142</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[6455200, 5227552]</td>\n",
       "      <td>3.90</td>\n",
       "      <td>18522274</td>\n",
       "      <td>US</td>\n",
       "      <td>Zoe Vanderveen is on the run with her captor t...</td>\n",
       "      <td>True</td>\n",
       "      <td>en-US</td>\n",
       "      <td>191</td>\n",
       "      <td>[25063023, 18553080, 17567752, 18126509, 17997...</td>\n",
       "      <td>6</td>\n",
       "      <td>Volition (The Perception Trilogy, #2)</td>\n",
       "      <td>6455200</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[187837]</td>\n",
       "      <td>3.19</td>\n",
       "      <td>17262776</td>\n",
       "      <td>US</td>\n",
       "      <td>The war is over, but for thirteen-year-old Rac...</td>\n",
       "      <td>True</td>\n",
       "      <td>eng</td>\n",
       "      <td>248</td>\n",
       "      <td>[16153997, 10836616, 17262238, 16074827, 13628...</td>\n",
       "      <td>68</td>\n",
       "      <td>Little Red Lies</td>\n",
       "      <td>187837</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               authors  average_rating   book_id country_code  \\\n",
       "3             [293603]            4.35  10099492           US   \n",
       "4            [4018722]            3.71  22642971           US   \n",
       "5            [6537142]            3.89  31556136           US   \n",
       "12  [6455200, 5227552]            3.90  18522274           US   \n",
       "13            [187837]            3.19  17262776           US   \n",
       "\n",
       "                                          description  is_ebook language_code  \\\n",
       "3   It all comes down to this.\\nVlad's running out...      True           eng   \n",
       "4   The future world is at peace.\\nElla Shepherd h...      True           eng   \n",
       "5   A gorgeously written and deeply felt literary ...      True                 \n",
       "12  Zoe Vanderveen is on the run with her captor t...      True         en-US   \n",
       "13  The war is over, but for thirteen-year-old Rac...      True           eng   \n",
       "\n",
       "    ratings_count                                      similar_books  \\\n",
       "3             152  [25861113, 7430195, 18765937, 6120544, 3247550...   \n",
       "4            1525  [20499652, 17934493, 13518102, 16210411, 17149...   \n",
       "5             109                                                 []   \n",
       "12            191  [25063023, 18553080, 17567752, 18126509, 17997...   \n",
       "13            248  [16153997, 10836616, 17262238, 16074827, 13628...   \n",
       "\n",
       "    text_reviews_count                                              title  \\\n",
       "3                    9  Twelfth Grade Kills (The Chronicles of Vladimi...   \n",
       "4                  428                                  The Body Electric   \n",
       "5                   45                                         Like Water   \n",
       "12                   6              Volition (The Perception Trilogy, #2)   \n",
       "13                  68                                    Little Red Lies   \n",
       "\n",
       "    first_author  book_idx  \n",
       "3         293603         0  \n",
       "4        4018722         1  \n",
       "5        6537142         2  \n",
       "12       6455200         3  \n",
       "13        187837         4  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import download_and_process_data\n",
    "\n",
    "(df_train, df_test, df_dev, df_valid), df_books = download_and_process_data()\n",
    "\n",
    "df_books.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at a sample of the labeled development set.\n",
    "As an example, we want our final recommendations model to be able to predict that a user who has interacted with `book_idxs` (25743, 22318, 7662, 6857, 83, 14495, 30664, ...) would either not read or not like the book with `book_idx` 22764 (first row), while a user who has interacted with `book_idxs` (3880, 18078, 9092, 29933, 1511, 8560, ...) would read and like the book with `book_idx` 3181 (second row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_idx</th>\n",
       "      <th>book_idxs</th>\n",
       "      <th>book_idx</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>159765</th>\n",
       "      <td>6236</td>\n",
       "      <td>(16739, 19021, 18074, 10278, 31179, 9850, 1198...</td>\n",
       "      <td>5174</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593901</th>\n",
       "      <td>22944</td>\n",
       "      <td>(29404, 19963, 19186, 13863, 29540, 7375, 2370...</td>\n",
       "      <td>13470</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199550</th>\n",
       "      <td>7732</td>\n",
       "      <td>(1101, 26775, 13761, 5265, 3269, 21272, 7424, ...</td>\n",
       "      <td>5912</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476454</th>\n",
       "      <td>18375</td>\n",
       "      <td>(23305, 24591, 221, 24819, 11955, 998, 16323, ...</td>\n",
       "      <td>10774</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248831</th>\n",
       "      <td>9615</td>\n",
       "      <td>(27162, 21741, 28317, 30890, 8239, 11392, 2499...</td>\n",
       "      <td>11392</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_idx                                          book_idxs  book_idx  \\\n",
       "159765      6236  (16739, 19021, 18074, 10278, 31179, 9850, 1198...      5174   \n",
       "593901     22944  (29404, 19963, 19186, 13863, 29540, 7375, 2370...     13470   \n",
       "199550      7732  (1101, 26775, 13761, 5265, 3269, 21272, 7424, ...      5912   \n",
       "476454     18375  (23305, 24591, 221, 24819, 11955, 998, 16323, ...     10774   \n",
       "248831      9615  (27162, 21741, 28317, 30890, 8239, 11392, 2499...     11392   \n",
       "\n",
       "        rating review_text  \n",
       "159765       0         NaN  \n",
       "593901       1         NaN  \n",
       "199550       1         NaN  \n",
       "476454       0         NaN  \n",
       "248831       1         NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev.sample(frac=1, random_state=12).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Labeling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVE = 1\n",
    "NEGATIVE = 0\n",
    "ABSTAIN = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a user has interacted with several books written by an author, there is a good chance that the user will read and like other books by the same author.\n",
    "We express this as a labeling function, using the `first_author` field in the `df_books` dataframe.\n",
    "We picked the threshold 15 by plotting histograms and running error analysis using the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling.lf import labeling_function\n",
    "\n",
    "book_to_first_author = dict(zip(df_books.book_idx, df_books.first_author))\n",
    "first_author_to_books_df = df_books.groupby(\"first_author\")[[\"book_idx\"]].agg(set)\n",
    "first_author_to_books = dict(\n",
    "    zip(first_author_to_books_df.index, first_author_to_books_df.book_idx)\n",
    ")\n",
    "\n",
    "\n",
    "@labeling_function(\n",
    "    resources=dict(\n",
    "        book_to_first_author=book_to_first_author,\n",
    "        first_author_to_books=first_author_to_books,\n",
    "    )\n",
    ")\n",
    "def shared_first_author(x, book_to_first_author, first_author_to_books):\n",
    "    author = book_to_first_author[x.book_idx]\n",
    "    same_author_books = first_author_to_books[author]\n",
    "    num_read = len(set(x.book_idxs).intersection(same_author_books))\n",
    "    return POSITIVE if num_read > 15 else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also leverage the long text reviews written by users to guess whether they liked or disliked a book.\n",
    "For example, the third `df_dev` entry above has a review with the text `'4.5 STARS'`, which indicates that the user liked the book.\n",
    "We write a simple LF that looks for similar phrases to guess the user's rating of a book.\n",
    "We interpret >= 4 stars to indicate a positive rating, while < 4 stars is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_rating_strs = [\n",
    "    \"one star\",\n",
    "    \"1 star\",\n",
    "    \"two star\",\n",
    "    \"2 star\",\n",
    "    \"3 star\",\n",
    "    \"three star\",\n",
    "    \"3.5 star\",\n",
    "    \"2.5 star\",\n",
    "    \"1 out of 5\",\n",
    "    \"2 out of 5\",\n",
    "    \"3 out of 5\",\n",
    "]\n",
    "high_rating_strs = [\"5 stars\", \"five stars\", \"four stars\", \"4 stars\", \"4.5 stars\"]\n",
    "\n",
    "\n",
    "@labeling_function(\n",
    "    resources=dict(low_rating_strs=low_rating_strs, high_rating_strs=high_rating_strs)\n",
    ")\n",
    "def stars_in_review(x, low_rating_strs, high_rating_strs):\n",
    "    if not isinstance(x.review_text, str):\n",
    "        return ABSTAIN\n",
    "    for low_rating_str in low_rating_strs:\n",
    "        if low_rating_str in x.review_text.lower():\n",
    "            return NEGATIVE\n",
    "    for high_rating_str in high_rating_strs:\n",
    "        if high_rating_str in x.review_text.lower():\n",
    "            return POSITIVE\n",
    "    return ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run [TextBlob](https://textblob.readthedocs.io/en/dev/index.html), a tool that provides a pretrained sentiment analyzer, on the reviews, and use its polarity and subjectivity scores to estimate the user's rating for the book.\n",
    "As usual, these thresholds were picked by analyzing the score distributions and running error analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.preprocess import preprocessor\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "@preprocessor(memoize=True)\n",
    "def textblob_polarity(x):\n",
    "    if isinstance(x.review_text, str):\n",
    "        x.blob = TextBlob(x.review_text)\n",
    "    else:\n",
    "        x.blob = None\n",
    "    return x\n",
    "\n",
    "\n",
    "# Label high polarity reviews as positive.\n",
    "@labeling_function(pre=[textblob_polarity])\n",
    "def polarity_positive(x):\n",
    "    if x.blob:\n",
    "        if x.blob.polarity > 0.3:\n",
    "            return POSITIVE\n",
    "    return ABSTAIN\n",
    "\n",
    "\n",
    "# Label high subjectivity reviews as positive.\n",
    "@labeling_function(pre=[textblob_polarity])\n",
    "def subjectivity_positive(x):\n",
    "    if x.blob:\n",
    "        if x.blob.subjectivity > 0.75:\n",
    "            return POSITIVE\n",
    "    return ABSTAIN\n",
    "\n",
    "\n",
    "# Label low polarity reviews as negative.\n",
    "@labeling_function(pre=[textblob_polarity])\n",
    "def polarity_negative(x):\n",
    "    if x.blob:\n",
    "        if x.blob.polarity < 0.0:\n",
    "            return NEGATIVE\n",
    "    return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "md-exclude-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:07<00:00, 1045.19it/s]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling import PandasLFApplier, LFAnalysis\n",
    "\n",
    "lfs = [\n",
    "    stars_in_review,\n",
    "    shared_first_author,\n",
    "    polarity_positive,\n",
    "    subjectivity_positive,\n",
    "    polarity_negative,\n",
    "]\n",
    "\n",
    "applier = PandasLFApplier(lfs)\n",
    "L_dev = applier.apply(df_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>stars_in_review</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.019500</td>\n",
       "      <td>0.004375</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>129</td>\n",
       "      <td>27</td>\n",
       "      <td>0.826923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shared_first_author</th>\n",
       "      <td>1</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.061875</td>\n",
       "      <td>0.002625</td>\n",
       "      <td>0.000750</td>\n",
       "      <td>335</td>\n",
       "      <td>160</td>\n",
       "      <td>0.676768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>polarity_positive</th>\n",
       "      <td>2</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>297</td>\n",
       "      <td>55</td>\n",
       "      <td>0.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subjectivity_positive</th>\n",
       "      <td>3</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.014625</td>\n",
       "      <td>0.003750</td>\n",
       "      <td>110</td>\n",
       "      <td>34</td>\n",
       "      <td>0.763889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>polarity_negative</th>\n",
       "      <td>4</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.019625</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.004375</td>\n",
       "      <td>87</td>\n",
       "      <td>70</td>\n",
       "      <td>0.554140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       j Polarity  Coverage  Overlaps  Conflicts  Correct  \\\n",
       "stars_in_review        0   [0, 1]  0.019500  0.004375   0.001375      129   \n",
       "shared_first_author    1      [1]  0.061875  0.002625   0.000750      335   \n",
       "polarity_positive      2      [1]  0.044000  0.014000   0.000625      297   \n",
       "subjectivity_positive  3      [1]  0.018000  0.014625   0.003750      110   \n",
       "polarity_negative      4      [0]  0.019625  0.005500   0.004375       87   \n",
       "\n",
       "                       Incorrect  Emp. Acc.  \n",
       "stars_in_review               27   0.826923  \n",
       "shared_first_author          160   0.676768  \n",
       "polarity_positive             55   0.843750  \n",
       "subjectivity_positive         34   0.763889  \n",
       "polarity_negative             70   0.554140  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LFAnalysis(L_dev, lfs).lf_summary(df_dev.rating.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying labeling functions to the training set\n",
    "\n",
    "We apply the labeling functions to the training set, and then filter out data points unlabeled by any LF to form our final training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "md-exclude-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 796319/796319 [12:43<00:00, 1043.34it/s]\n",
      "INFO:root:Computing O...\n",
      "INFO:root:Estimating \\mu...\n",
      "INFO:root:[0 epochs]: TRAIN:[loss=0.002]\n",
      "INFO:root:[20 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[40 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[60 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[80 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[100 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[120 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[140 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[160 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[180 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[200 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[220 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[240 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[260 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[280 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[300 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[320 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[340 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[360 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[380 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[400 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[420 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[440 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[460 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[480 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[500 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[520 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[540 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[560 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[580 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[600 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[620 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[640 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[660 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[680 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[700 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[720 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[740 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[760 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[780 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[800 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[820 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[840 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[860 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[880 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[900 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[920 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[940 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[960 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[980 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[1000 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[1020 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[1040 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[1060 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[1080 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[1100 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[1120 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[1140 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[1160 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[1180 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[1200 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[1220 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[1240 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[1260 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[1280 epochs]: TRAIN:[loss=0.000]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling.model import LabelModel\n",
    "\n",
    "L_train = applier.apply(df_train)\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L_train, n_epochs=5000, seed=123, log_freq=20, lr=0.01)\n",
    "preds_train = label_model.predict(L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "md-exclude-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/snorkel-tutorials/.tox/recsys/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling import filter_unlabeled_dataframe\n",
    "\n",
    "df_train_filtered, preds_train_filtered = filter_unlabeled_dataframe(\n",
    "    df_train, preds_train, L_train\n",
    ")\n",
    "df_train_filtered[\"rating\"] = preds_train_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rating Prediction Model\n",
    "We write a Keras model for predicting ratings given a user's book list and a book (which is being rated).\n",
    "The model represents the list of books the user interacted with, `books_idxs`, by learning an embedding for each idx, and averaging the embeddings in `book_idxs`.\n",
    "It learns another embedding for the `book_idx`, the book to be rated.\n",
    "Then it concatenates the two embeddings and uses an [MLP](https://en.wikipedia.org/wiki/Multilayer_perceptron) to compute the probability of the `rating` being 1.\n",
    "This type of model is common in large-scale recommender systems, for example, the [YouTube recommender system](https://ai.google/research/pubs/pub45530)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from utils import precision_batch, recall_batch, f1_batch\n",
    "\n",
    "n_books = max([max(df.book_idx) for df in [df_train, df_test, df_dev, df_valid]])\n",
    "\n",
    "\n",
    "# Keras model to predict rating given book_idxs and book_idx.\n",
    "def get_model(embed_dim=64, hidden_layer_sizes=[32]):\n",
    "    # Compute embedding for book_idxs.\n",
    "    len_book_idxs = tf.keras.layers.Input([])\n",
    "    book_idxs = tf.keras.layers.Input([None])\n",
    "    # book_idxs % n_books is to prevent crashing if a book_idx in book_idxs is > n_books.\n",
    "    book_idxs_emb = tf.keras.layers.Embedding(n_books, embed_dim)(book_idxs % n_books)\n",
    "    book_idxs_emb = tf.math.divide(\n",
    "        tf.keras.backend.sum(book_idxs_emb, axis=1), tf.expand_dims(len_book_idxs, 1)\n",
    "    )\n",
    "    # Compute embedding for book_idx.\n",
    "    book_idx = tf.keras.layers.Input([])\n",
    "    book_idx_emb = tf.keras.layers.Embedding(n_books, embed_dim)(book_idx)\n",
    "    input_layer = tf.keras.layers.concatenate([book_idxs_emb, book_idx_emb], 1)\n",
    "    # Build Multi Layer Perceptron on input layer.\n",
    "    cur_layer = input_layer\n",
    "    for size in hidden_layer_sizes:\n",
    "        tf.keras.layers.Dense(size, activation=tf.nn.relu)(cur_layer)\n",
    "    output_layer = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)(cur_layer)\n",
    "    # Create and compile keras model.\n",
    "    model = tf.keras.Model(\n",
    "        inputs=[len_book_idxs, book_idxs, book_idx], outputs=[output_layer]\n",
    "    )\n",
    "    model.compile(\n",
    "        \"Adagrad\",\n",
    "        \"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\", f1_batch, precision_batch, recall_batch],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use triples of (`book_idxs`, `book_idx`, `rating`) from our dataframes as training data points. In addition, we want to train the model to recognize when a user will not read a book. To create data points for that, we randomly sample a `book_id` not in `book_idxs` and use that with a `rating` of 0 as a _random negative_ data point. We create one such _random negative_ data point for every positive (`rating` 1) data point in our dataframe so that positive and negative data points are roughly balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator to turn dataframe into data points.\n",
    "def get_data_points_generator(df):\n",
    "    def generator():\n",
    "        for book_idxs, book_idx, rating in zip(df.book_idxs, df.book_idx, df.rating):\n",
    "            # Remove book_idx from book_idxs so the model can't just look it up.\n",
    "            book_idxs = tuple(filter(lambda x: x != book_idx, book_idxs))\n",
    "            yield {\n",
    "                \"len_book_idxs\": len(book_idxs),\n",
    "                \"book_idxs\": book_idxs,\n",
    "                \"book_idx\": book_idx,\n",
    "                \"label\": rating,\n",
    "            }\n",
    "            if rating == 1:\n",
    "                # Generate a random negative book_id not in book_idxs.\n",
    "                random_negative = np.random.randint(0, n_books)\n",
    "                while random_negative in book_idxs:\n",
    "                    random_negative = np.random.randint(0, n_books)\n",
    "                yield {\n",
    "                    \"len_book_idxs\": len(book_idxs),\n",
    "                    \"book_idxs\": book_idxs,\n",
    "                    \"book_idx\": random_negative,\n",
    "                    \"label\": 0,\n",
    "                }\n",
    "\n",
    "    return generator\n",
    "\n",
    "\n",
    "def get_data_tensors(df):\n",
    "    # Use generator to get data points each epoch, along with shuffling and batching.\n",
    "    padded_shapes = {\n",
    "        \"len_book_idxs\": [],\n",
    "        \"book_idxs\": [None],\n",
    "        \"book_idx\": [],\n",
    "        \"label\": [],\n",
    "    }\n",
    "    dataset = (\n",
    "        tf.data.Dataset.from_generator(\n",
    "            get_data_points_generator(df), {k: tf.int64 for k in padded_shapes}\n",
    "        )\n",
    "        .shuffle(123)\n",
    "        .repeat(None)\n",
    "        .padded_batch(batch_size=256, padded_shapes=padded_shapes)\n",
    "    )\n",
    "    tensor_dict = tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n",
    "    return (\n",
    "        (\n",
    "            tensor_dict[\"len_book_idxs\"],\n",
    "            tensor_dict[\"book_idxs\"],\n",
    "            tensor_dict[\"book_idx\"],\n",
    "        ),\n",
    "        tensor_dict[\"label\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We now train the model on our combined training data (data labeled by LFs plus dev data).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "md-exclude-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/snorkel-tutorials/.tox/recsys/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/ubuntu/snorkel-tutorials/.tox/recsys/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/ubuntu/snorkel-tutorials/.tox/recsys/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/ubuntu/snorkel-tutorials/.tox/recsys/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py:494: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "WARNING:tensorflow:From /home/ubuntu/snorkel-tutorials/.tox/recsys/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/adagrad.py:105: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "300/300 [==============================] - 14s 47ms/step - loss: 0.6749 - acc: 0.6114 - f1_batch: 0.1111 - precision_batch: 0.5797 - recall_batch: 0.0716 - val_loss: 0.6804 - val_acc: 0.6060 - val_f1_batch: 0.1306 - val_precision_batch: 0.4676 - val_recall_batch: 0.0836\n",
      "Epoch 2/30\n",
      "300/300 [==============================] - 14s 46ms/step - loss: 0.6640 - acc: 0.6277 - f1_batch: 0.1212 - precision_batch: 0.7633 - recall_batch: 0.0683 - val_loss: 0.6776 - val_acc: 0.6185 - val_f1_batch: 0.1311 - val_precision_batch: 0.5143 - val_recall_batch: 0.0830\n",
      "Epoch 3/30\n",
      "300/300 [==============================] - 14s 46ms/step - loss: 0.6548 - acc: 0.6373 - f1_batch: 0.1667 - precision_batch: 0.8191 - recall_batch: 0.0956 - val_loss: 0.6782 - val_acc: 0.6159 - val_f1_batch: 0.1784 - val_precision_batch: 0.5179 - val_recall_batch: 0.1174\n",
      "Epoch 4/30\n",
      "300/300 [==============================] - 14s 45ms/step - loss: 0.6490 - acc: 0.6528 - f1_batch: 0.2255 - precision_batch: 0.8556 - recall_batch: 0.1343 - val_loss: 0.6766 - val_acc: 0.6239 - val_f1_batch: 0.2079 - val_precision_batch: 0.5847 - val_recall_batch: 0.1391\n",
      "Epoch 5/30\n",
      "300/300 [==============================] - 14s 46ms/step - loss: 0.6391 - acc: 0.6629 - f1_batch: 0.2674 - precision_batch: 0.8516 - recall_batch: 0.1633 - val_loss: 0.6751 - val_acc: 0.6239 - val_f1_batch: 0.2451 - val_precision_batch: 0.5956 - val_recall_batch: 0.1690\n",
      "Epoch 6/30\n",
      "300/300 [==============================] - 14s 46ms/step - loss: 0.6378 - acc: 0.6688 - f1_batch: 0.3091 - precision_batch: 0.8569 - recall_batch: 0.1942 - val_loss: 0.6743 - val_acc: 0.6249 - val_f1_batch: 0.2512 - val_precision_batch: 0.5329 - val_recall_batch: 0.1776\n",
      "Epoch 7/30\n",
      "300/300 [==============================] - 14s 46ms/step - loss: 0.6253 - acc: 0.6868 - f1_batch: 0.3471 - precision_batch: 0.8615 - recall_batch: 0.2227 - val_loss: 0.6724 - val_acc: 0.6282 - val_f1_batch: 0.2804 - val_precision_batch: 0.5790 - val_recall_batch: 0.2019\n",
      "Epoch 8/30\n",
      "300/300 [==============================] - 14s 46ms/step - loss: 0.6274 - acc: 0.6852 - f1_batch: 0.3760 - precision_batch: 0.8628 - recall_batch: 0.2461 - val_loss: 0.6737 - val_acc: 0.6323 - val_f1_batch: 0.3091 - val_precision_batch: 0.5801 - val_recall_batch: 0.2284\n",
      "Epoch 9/30\n",
      "300/300 [==============================] - 14s 45ms/step - loss: 0.6146 - acc: 0.7025 - f1_batch: 0.4071 - precision_batch: 0.8616 - recall_batch: 0.2720 - val_loss: 0.6718 - val_acc: 0.6332 - val_f1_batch: 0.3362 - val_precision_batch: 0.5985 - val_recall_batch: 0.2535\n",
      "Epoch 10/30\n",
      "300/300 [==============================] - 14s 46ms/step - loss: 0.6130 - acc: 0.7068 - f1_batch: 0.4390 - precision_batch: 0.8694 - recall_batch: 0.2990 - val_loss: 0.6716 - val_acc: 0.6393 - val_f1_batch: 0.3325 - val_precision_batch: 0.5808 - val_recall_batch: 0.2488\n",
      "Epoch 11/30\n",
      "300/300 [==============================] - 13s 45ms/step - loss: 0.6076 - acc: 0.7140 - f1_batch: 0.4598 - precision_batch: 0.8610 - recall_batch: 0.3197 - val_loss: 0.6679 - val_acc: 0.6423 - val_f1_batch: 0.3665 - val_precision_batch: 0.5924 - val_recall_batch: 0.2861\n",
      "Epoch 12/30\n",
      "300/300 [==============================] - 14s 46ms/step - loss: 0.6030 - acc: 0.7176 - f1_batch: 0.4770 - precision_batch: 0.8581 - recall_batch: 0.3355 - val_loss: 0.6722 - val_acc: 0.6399 - val_f1_batch: 0.3712 - val_precision_batch: 0.5784 - val_recall_batch: 0.2895\n",
      "Epoch 13/30\n",
      "300/300 [==============================] - 14s 45ms/step - loss: 0.5986 - acc: 0.7265 - f1_batch: 0.5010 - precision_batch: 0.8583 - recall_batch: 0.3595 - val_loss: 0.6688 - val_acc: 0.6468 - val_f1_batch: 0.3990 - val_precision_batch: 0.5992 - val_recall_batch: 0.3163\n",
      "Epoch 14/30\n",
      "300/300 [==============================] - 14s 45ms/step - loss: 0.5911 - acc: 0.7332 - f1_batch: 0.5188 - precision_batch: 0.8555 - recall_batch: 0.3773 - val_loss: 0.6677 - val_acc: 0.6472 - val_f1_batch: 0.3832 - val_precision_batch: 0.5890 - val_recall_batch: 0.2992\n",
      "Epoch 15/30\n",
      "300/300 [==============================] - 14s 45ms/step - loss: 0.5922 - acc: 0.7338 - f1_batch: 0.5334 - precision_batch: 0.8534 - recall_batch: 0.3938 - val_loss: 0.6688 - val_acc: 0.6459 - val_f1_batch: 0.4080 - val_precision_batch: 0.5860 - val_recall_batch: 0.3333\n",
      "Epoch 16/30\n",
      "300/300 [==============================] - 14s 45ms/step - loss: 0.5796 - acc: 0.7456 - f1_batch: 0.5491 - precision_batch: 0.8516 - recall_batch: 0.4108 - val_loss: 0.6713 - val_acc: 0.6419 - val_f1_batch: 0.4107 - val_precision_batch: 0.5861 - val_recall_batch: 0.3321\n",
      "Epoch 17/30\n",
      "300/300 [==============================] - 14s 45ms/step - loss: 0.5863 - acc: 0.7424 - f1_batch: 0.5611 - precision_batch: 0.8559 - recall_batch: 0.4220 - val_loss: 0.6667 - val_acc: 0.6518 - val_f1_batch: 0.4308 - val_precision_batch: 0.5872 - val_recall_batch: 0.3563\n",
      "Epoch 18/30\n",
      "300/300 [==============================] - 13s 45ms/step - loss: 0.5715 - acc: 0.7555 - f1_batch: 0.5748 - precision_batch: 0.8529 - recall_batch: 0.4394 - val_loss: 0.6661 - val_acc: 0.6503 - val_f1_batch: 0.4248 - val_precision_batch: 0.5792 - val_recall_batch: 0.3520\n",
      "Epoch 19/30\n",
      "300/300 [==============================] - 14s 45ms/step - loss: 0.5746 - acc: 0.7550 - f1_batch: 0.5897 - precision_batch: 0.8572 - recall_batch: 0.4550 - val_loss: 0.6674 - val_acc: 0.6478 - val_f1_batch: 0.4373 - val_precision_batch: 0.5818 - val_recall_batch: 0.3670\n",
      "Epoch 20/30\n",
      "300/300 [==============================] - 14s 45ms/step - loss: 0.5684 - acc: 0.7604 - f1_batch: 0.5984 - precision_batch: 0.8521 - recall_batch: 0.4664 - val_loss: 0.6698 - val_acc: 0.6533 - val_f1_batch: 0.4582 - val_precision_batch: 0.6000 - val_recall_batch: 0.3850\n",
      "Epoch 21/30\n",
      "300/300 [==============================] - 14s 45ms/step - loss: 0.5658 - acc: 0.7618 - f1_batch: 0.6050 - precision_batch: 0.8528 - recall_batch: 0.4735 - val_loss: 0.6648 - val_acc: 0.6556 - val_f1_batch: 0.4570 - val_precision_batch: 0.5833 - val_recall_batch: 0.3920\n",
      "Epoch 22/30\n",
      "300/300 [==============================] - 13s 45ms/step - loss: 0.5624 - acc: 0.7650 - f1_batch: 0.6121 - precision_batch: 0.8455 - recall_batch: 0.4849 - val_loss: 0.6658 - val_acc: 0.6517 - val_f1_batch: 0.4444 - val_precision_batch: 0.5772 - val_recall_batch: 0.3771\n",
      "Epoch 23/30\n",
      "300/300 [==============================] - 14s 45ms/step - loss: 0.5589 - acc: 0.7675 - f1_batch: 0.6244 - precision_batch: 0.8468 - recall_batch: 0.4989 - val_loss: 0.6668 - val_acc: 0.6498 - val_f1_batch: 0.4591 - val_precision_batch: 0.5772 - val_recall_batch: 0.3958\n",
      "Epoch 24/30\n",
      "300/300 [==============================] - 14s 45ms/step - loss: 0.5581 - acc: 0.7694 - f1_batch: 0.6288 - precision_batch: 0.8452 - recall_batch: 0.5056 - val_loss: 0.6719 - val_acc: 0.6493 - val_f1_batch: 0.4793 - val_precision_batch: 0.5901 - val_recall_batch: 0.4184\n",
      "Epoch 25/30\n",
      "300/300 [==============================] - 14s 45ms/step - loss: 0.5464 - acc: 0.7771 - f1_batch: 0.6361 - precision_batch: 0.8424 - recall_batch: 0.5157 - val_loss: 0.6637 - val_acc: 0.6543 - val_f1_batch: 0.4676 - val_precision_batch: 0.5662 - val_recall_batch: 0.4101\n",
      "Epoch 26/30\n",
      "300/300 [==============================] - 14s 46ms/step - loss: 0.5566 - acc: 0.7709 - f1_batch: 0.6417 - precision_batch: 0.8425 - recall_batch: 0.5226 - val_loss: 0.6642 - val_acc: 0.6573 - val_f1_batch: 0.4693 - val_precision_batch: 0.5848 - val_recall_batch: 0.4086\n",
      "Epoch 27/30\n",
      "300/300 [==============================] - 14s 46ms/step - loss: 0.5400 - acc: 0.7806 - f1_batch: 0.6453 - precision_batch: 0.8408 - recall_batch: 0.5281 - val_loss: 0.6661 - val_acc: 0.6510 - val_f1_batch: 0.4703 - val_precision_batch: 0.5764 - val_recall_batch: 0.4121\n",
      "Epoch 28/30\n",
      "300/300 [==============================] - 14s 46ms/step - loss: 0.5461 - acc: 0.7790 - f1_batch: 0.6545 - precision_batch: 0.8452 - recall_batch: 0.5383 - val_loss: 0.6673 - val_acc: 0.6542 - val_f1_batch: 0.4991 - val_precision_batch: 0.5848 - val_recall_batch: 0.4482\n",
      "Epoch 29/30\n",
      "300/300 [==============================] - 14s 45ms/step - loss: 0.5393 - acc: 0.7833 - f1_batch: 0.6586 - precision_batch: 0.8432 - recall_batch: 0.5449 - val_loss: 0.6691 - val_acc: 0.6505 - val_f1_batch: 0.4698 - val_precision_batch: 0.5555 - val_recall_batch: 0.4190\n",
      "Epoch 30/30\n",
      "300/300 [==============================] - 14s 45ms/step - loss: 0.5391 - acc: 0.7814 - f1_batch: 0.6593 - precision_batch: 0.8376 - recall_batch: 0.5475 - val_loss: 0.6641 - val_acc: 0.6554 - val_f1_batch: 0.4845 - val_precision_batch: 0.5795 - val_recall_batch: 0.4302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fac202d3ef0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import get_n_epochs\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "X_train, Y_train = get_data_tensors(df_train_filtered)\n",
    "X_valid, Y_valid = get_data_tensors(df_valid)\n",
    "model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    steps_per_epoch=300,\n",
    "    validation_data=(X_valid, Y_valid),\n",
    "    validation_steps=40,\n",
    "    epochs=get_n_epochs(),\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Finally, we evaluate the model's predicted ratings on our test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 1s 33ms/step - loss: 0.6534 - acc: 0.6570 - f1_batch: 0.5226 - precision_batch: 0.5720 - recall_batch: 0.4931\n"
     ]
    }
   ],
   "source": [
    "X_test, Y_test = get_data_tensors(df_test)\n",
    "_ = model.evaluate(X_test, Y_test, steps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model has generalized quite well to our test set!\n",
    "Note that we should additionally measure ranking metrics, like precision@10, before deploying to production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we showed one way to use Snorkel for recommendations.\n",
    "We used book metadata and review text to create LFs that estimate user ratings.\n",
    "We used Snorkel's `LabelModel` to combine the outputs of those LFs.\n",
    "Finally, we trained a model to predict whether a user will read and like a given book (and therefore what books should be recommended to the user) based only on what books the user has interacted with in the past.\n",
    "\n",
    "Here we demonstrated one way to use Snorkel for training a recommender system.\n",
    "Note, however, that this approach could easily be adapted to take advantage of additional information as it is available (e.g., user profile data, denser user ratings, and so on.)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,-all"
  },
  "kernelspec": {
   "display_name": "snorkel37",
   "language": "python",
   "name": "snorkel37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
