{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Chest X-rays with Cross-Modal Data Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates how to use the *cross-modal data programming* technique described in Dunnmon and Ratner, et al. (2019) to build a Convolutional Neural Network (CNN) model with no hand-labeled data that performs similarly to a CNN supervised using several thousand data points labeled by radiologists.  This process is *exactly* equivalent to that followed for the chest radiograph dataset in our 2019 Nature submission.\n",
    "\n",
    "We begin by setting up our environment, importing relevant Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# Importing pandas for data processing\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Loading and Splitting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we set up the data dictionary and load data that we've already split for you into an (approximately) 80% train split, 10% development split, and 10% test split.  Each raw data point contains three fields: a text report, a label (normal or abnormal), and a set of image paths.  The original data, from the OpenI dataset, is maintained by [NIH](https://openi.nlm.nih.gov/faq.php)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2630 train examples: 63.8% Abnormal\n",
      "376 dev examples: 63.0% Abnormal\n",
      "378 test examples: 61.6% Abnormal\n"
     ]
    }
   ],
   "source": [
    "# Setting up data dictionary and defining data splits\n",
    "data = {}\n",
    "splits = [\"train\", \"dev\", \"test\"]\n",
    "\n",
    "for split in splits:\n",
    "    data[split] = pd.read_csv(f\"data/{split}_entries.csv\")[\n",
    "        [\"label\", \"xray_paths\", \"text\"]\n",
    "    ]\n",
    "    # Adjusting labels to fit with Snorkel MeTaL labeling convention\n",
    "    data[split][\"label\"][data[split][\"label\"] == 0] = 2\n",
    "    perc_pos = sum(data[split][\"label\"] == 1) / len(data[split])\n",
    "    print(f\"{len(data[split])} {split} examples: {100*perc_pos:0.1f}% Abnormal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see an example of a single data point below -- note that the raw label convention for our normal vs. abnormal classification problem is 1 for abnormal and 2 for abnormal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW TEXT:\n",
      " \n",
      " COMPARISON: Chest x-XXXX XXXX INDICATION: XXXX in bathtub FINDINGS: The lungs and pleural spaces show no acute abnormality. Hyperexpanded lungs. Calcified right upper lobe granuloma, unchanged. Heart size and pulmonary vascularity within normal limits. No displaced rib fractures. IMPRESSION: 1. Hyperexpansion without acute pulmonary abnormality. \n",
      "\n",
      "IMAGE PATHS: \n",
      " \n",
      " ./data/openi/xrays/CXR2824_IM-1245-13001.png \n",
      "\n",
      "LABEL: 1\n"
     ]
    }
   ],
   "source": [
    "sample = data[\"train\"].iloc[0]\n",
    "print(\"RAW TEXT:\\n \\n\", sample[\"text\"], \"\\n\")\n",
    "print(\"IMAGE PATHS: \\n \\n\", sample[\"xray_paths\"], \"\\n\")\n",
    "print(\"LABEL:\", sample[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Developing LFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define our *labeling functions* (LFs): simple, heuristic functions written by a domain expert (e.g., a radiologist) that correctly label a report as normal or abnormal with probability better than random chance.\n",
    "\n",
    "We give an example of all three types of LFs we reference in the paper: general pattern LFs that operate on patterns a non-expert user could easily identify, medical pattern LFs that operate on patterns easily identifiable by a clinician, and structural LFs that focus on specific structural elements of the report (e.g. how long it is) that have some correlation with the scan it describes being normal or abnormal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Value to use for abstain votes\n",
    "ABSTAIN = 0\n",
    "# Value to use for abnormal votes\n",
    "ABNORMAL = 1\n",
    "# Value to user for normal votes\n",
    "NORMAL = 2\n",
    "\n",
    "\n",
    "# Example of a General Pattern LF\n",
    "def LF_is_seen_or_noted_in_report_demo(report):\n",
    "    if any(word in report.lower() for word in [\"is seen\", \"noted\"]):\n",
    "        return ABNORMAL\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "\n",
    "# Example of a Medical Pattern LF\n",
    "def LF_lung_hyperdistention_demo(report):\n",
    "    \"\"\"\n",
    "    Votes abnormal for indications of lung hyperdistention.\n",
    "    \"\"\"\n",
    "    reg_01 = re.compile(\"increased volume|hyperexpan|inflated\", re.IGNORECASE)\n",
    "    for s in report.split(\".\"):\n",
    "        if reg_01.search(s):\n",
    "            return ABNORMAL\n",
    "    ### *** ###\n",
    "    return NORMAL\n",
    "\n",
    "\n",
    "# Example of a Structural LF\n",
    "def LF_report_is_short_demo(report):\n",
    "    \"\"\"\n",
    "    Checks if report is short.\n",
    "    \"\"\"\n",
    "    return NORMAL if len(report) < 280 else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see how well these LFs might do at correctly indicating normal or abnormal examples.  Check them out by changing the `lf_test` function in the cell below to reference one of those listed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>168</td>\n",
       "      <td>208</td>\n",
       "      <td>0.446809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Polarity  Coverage  Correct  Incorrect  Emp. Acc.\n",
       "0   [1, 2]       1.0      168        208   0.446809"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from snorkel.labeling.analysis import single_lf_summary, confusion_matrix\n",
    "\n",
    "# Testing single LF\n",
    "lf_test = LF_lung_hyperdistention_demo\n",
    "\n",
    "# Computing labels\n",
    "Y_lf = np.array([lf_test(doc[\"text\"]) for ind, doc in data[\"dev\"].iterrows()])\n",
    "Y_dev = np.array([doc[\"label\"] for ind, doc in data[\"dev\"].iterrows()])\n",
    "\n",
    "# Summarizing LF performance\n",
    "single_lf_summary(Y_lf, Y=Y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use analyze the `LF_lung_hyperdistention_demo` function -- in this case,  we see that it has polarity [1,2], meaning it votes on both class 1 and class 2 (and votes on every example because `coverage` = 1.0), but that it has low accuracy (around 44%).  Let's look at the confusion matrix to see why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        y=1   \n",
      " l=1    136   \n"
     ]
    }
   ],
   "source": [
    "# Print confusion matrix\n",
    "conf = confusion_matrix(Y_dev, Y_lf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, this LF is much more accurate on abnormal examples (where y=1) than on abnormal examples (where y=2).  Why don't we adjust it to only vote in the positive direction and see how we do?\n",
    "\n",
    "Go ahead and change `NORMAL` to `ABSTAIN` in the `LF_lung_hyperdistention_demo` function (the line below the `### *** ###` comment), and rerun the last three code cells.\n",
    "\n",
    "You'll see that by making this rule a bit more targeted, its coverage decreases to 9%, but it's accuracy jumps to over 90%.  This type of iteration is exactly how clinicians can develop LFs in practice.\n",
    "\n",
    "You may also notice that it's very easy to write these LFs over text, but it would be very hard to, say, write an `LF_lung_hyperdistention` version that operates over an image -- this is why cross-modality is so important!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Computing the Label Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've designed a couple of LFs, it's time to execute them all on every example we have to create a *label matrix*.  This is an $n$ by $m$ matrix, where $n$ is the number of examples and $m$ is the number of LFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from labeling_functions import (\n",
    "    LF_report_is_short,\n",
    "    LF_consistency_in_report,\n",
    "    LF_negative_inflection_words_in_report,\n",
    "    LF_is_seen_or_noted_in_report,\n",
    "    LF_disease_in_report,\n",
    "    LF_abnormal_mesh_terms_in_report,\n",
    "    LF_recommend_in_report,\n",
    "    LF_mm_in_report,\n",
    "    LF_normal,\n",
    "    LF_positive_MeshTerm,\n",
    "    LF_fracture,\n",
    "    LF_calcinosis,\n",
    "    LF_degen_spine,\n",
    "    LF_lung_hypoinflation,\n",
    "    LF_lung_hyperdistention,\n",
    "    LF_catheters,\n",
    "    LF_surgical,\n",
    "    LF_granuloma,\n",
    ")\n",
    "\n",
    "lfs = [\n",
    "    LF_report_is_short,\n",
    "    LF_consistency_in_report,\n",
    "    LF_negative_inflection_words_in_report,\n",
    "    LF_is_seen_or_noted_in_report,\n",
    "    LF_disease_in_report,\n",
    "    LF_abnormal_mesh_terms_in_report,\n",
    "    LF_recommend_in_report,\n",
    "    LF_mm_in_report,\n",
    "    LF_normal,\n",
    "    LF_positive_MeshTerm,\n",
    "    LF_fracture,\n",
    "    LF_calcinosis,\n",
    "    LF_degen_spine,\n",
    "    LF_lung_hypoinflation,\n",
    "    LF_lung_hyperdistention,\n",
    "    LF_catheters,\n",
    "    LF_surgical,\n",
    "    LF_granuloma,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a few simple helper functions for running our labeling functions over all text reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "def evaluate_lf_on_docs(docs, lf):\n",
    "    \"\"\"\n",
    "    Evaluates lf on list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    lf_list = []\n",
    "    for doc in docs:\n",
    "        lf_list.append(lf(doc))\n",
    "    return lf_list\n",
    "\n",
    "\n",
    "def create_label_matrix(lfs, docs):\n",
    "    \"\"\"\n",
    "    Creates label matrix from documents and lfs\n",
    "    \"\"\"\n",
    "\n",
    "    delayed_lf_rows = []\n",
    "\n",
    "    for lf in lfs:\n",
    "        delayed_lf_rows.append(dask.delayed(evaluate_lf_on_docs)(docs, lf))\n",
    "\n",
    "    with ProgressBar():\n",
    "        L = csr_matrix(np.vstack(dask.compute(*delayed_lf_rows)).transpose())\n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we simply apply each of our LFs to each of our reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing label matrices...\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "Creating label vectors...\n"
     ]
    }
   ],
   "source": [
    "# Get lf names\n",
    "lf_names = [lf.__name__ for lf in lfs]\n",
    "\n",
    "# Allocating label matrix and ground truth label lists\n",
    "Ls = []\n",
    "Ys = []\n",
    "\n",
    "# Computing lfs\n",
    "print(\"Computing label matrices...\")\n",
    "for i, docs in enumerate(\n",
    "    (\n",
    "        data[\"train\"][\"text\"].tolist(),\n",
    "        data[\"dev\"][\"text\"].tolist(),\n",
    "        data[\"test\"][\"text\"].tolist(),\n",
    "    )\n",
    "):\n",
    "    Ls.append(create_label_matrix(lfs, docs))\n",
    "\n",
    "# Getting ground truth labels\n",
    "print(\"Creating label vectors...\")\n",
    "Ys = [\n",
    "    data[\"train\"][\"label\"].tolist(),\n",
    "    data[\"dev\"][\"label\"].tolist(),\n",
    "    data[\"test\"][\"label\"].tolist(),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've done this, we can inspect our accuracy on the development set and other useful LF metrics using the simple MeTaL interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LF_report_is_short</th>\n",
       "      <td>0</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.303191</td>\n",
       "      <td>0.303191</td>\n",
       "      <td>0.207447</td>\n",
       "      <td>72</td>\n",
       "      <td>42</td>\n",
       "      <td>0.631579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_consistency_in_report</th>\n",
       "      <td>1</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.928191</td>\n",
       "      <td>0.401596</td>\n",
       "      <td>305</td>\n",
       "      <td>71</td>\n",
       "      <td>0.811170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_negative_inflection_words_in_report</th>\n",
       "      <td>2</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.082447</td>\n",
       "      <td>0.082447</td>\n",
       "      <td>0.029255</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.967742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_is_seen_or_noted_in_report</th>\n",
       "      <td>3</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.026596</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>0.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_disease_in_report</th>\n",
       "      <td>4</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.372340</td>\n",
       "      <td>0.372340</td>\n",
       "      <td>0.175532</td>\n",
       "      <td>89</td>\n",
       "      <td>51</td>\n",
       "      <td>0.635714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_abnormal_mesh_terms_in_report</th>\n",
       "      <td>5</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.417553</td>\n",
       "      <td>0.417553</td>\n",
       "      <td>0.138298</td>\n",
       "      <td>141</td>\n",
       "      <td>16</td>\n",
       "      <td>0.898089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_recommend_in_report</th>\n",
       "      <td>6</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.015957</td>\n",
       "      <td>0.015957</td>\n",
       "      <td>0.002660</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_mm_in_report</th>\n",
       "      <td>7</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.053191</td>\n",
       "      <td>0.053191</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_normal</th>\n",
       "      <td>8</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.159574</td>\n",
       "      <td>0.159574</td>\n",
       "      <td>0.114362</td>\n",
       "      <td>34</td>\n",
       "      <td>26</td>\n",
       "      <td>0.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_positive_MeshTerm</th>\n",
       "      <td>9</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.295213</td>\n",
       "      <td>0.295213</td>\n",
       "      <td>0.106383</td>\n",
       "      <td>108</td>\n",
       "      <td>3</td>\n",
       "      <td>0.972973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_fracture</th>\n",
       "      <td>10</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.031915</td>\n",
       "      <td>0.031915</td>\n",
       "      <td>0.013298</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_calcinosis</th>\n",
       "      <td>11</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.018617</td>\n",
       "      <td>0.018617</td>\n",
       "      <td>0.007979</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_degen_spine</th>\n",
       "      <td>12</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.111702</td>\n",
       "      <td>0.111702</td>\n",
       "      <td>0.015957</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_lung_hypoinflation</th>\n",
       "      <td>13</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.069149</td>\n",
       "      <td>0.069149</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_lung_hyperdistention</th>\n",
       "      <td>14</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.093085</td>\n",
       "      <td>0.093085</td>\n",
       "      <td>0.031915</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0.914286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_catheters</th>\n",
       "      <td>15</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.034574</td>\n",
       "      <td>0.034574</td>\n",
       "      <td>0.007979</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_surgical</th>\n",
       "      <td>16</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.029255</td>\n",
       "      <td>0.029255</td>\n",
       "      <td>0.005319</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_granuloma</th>\n",
       "      <td>17</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.140957</td>\n",
       "      <td>0.140957</td>\n",
       "      <td>0.061170</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         j Polarity  Coverage  Overlaps  \\\n",
       "LF_report_is_short                       0      [2]  0.303191  0.303191   \n",
       "LF_consistency_in_report                 1   [1, 2]  1.000000  0.928191   \n",
       "LF_negative_inflection_words_in_report   2      [1]  0.082447  0.082447   \n",
       "LF_is_seen_or_noted_in_report            3      [1]  0.085106  0.085106   \n",
       "LF_disease_in_report                     4      [1]  0.372340  0.372340   \n",
       "LF_abnormal_mesh_terms_in_report         5      [1]  0.417553  0.417553   \n",
       "LF_recommend_in_report                   6      [1]  0.015957  0.015957   \n",
       "LF_mm_in_report                          7      [1]  0.053191  0.053191   \n",
       "LF_normal                                8      [2]  0.159574  0.159574   \n",
       "LF_positive_MeshTerm                     9      [1]  0.295213  0.295213   \n",
       "LF_fracture                             10      [1]  0.031915  0.031915   \n",
       "LF_calcinosis                           11      [1]  0.018617  0.018617   \n",
       "LF_degen_spine                          12      [1]  0.111702  0.111702   \n",
       "LF_lung_hypoinflation                   13      [1]  0.069149  0.069149   \n",
       "LF_lung_hyperdistention                 14      [1]  0.093085  0.093085   \n",
       "LF_catheters                            15      [1]  0.034574  0.034574   \n",
       "LF_surgical                             16      [1]  0.029255  0.029255   \n",
       "LF_granuloma                            17      [1]  0.140957  0.140957   \n",
       "\n",
       "                                        Conflicts  Correct  Incorrect  \\\n",
       "LF_report_is_short                       0.207447       72         42   \n",
       "LF_consistency_in_report                 0.401596      305         71   \n",
       "LF_negative_inflection_words_in_report   0.029255       30          1   \n",
       "LF_is_seen_or_noted_in_report            0.026596       25          7   \n",
       "LF_disease_in_report                     0.175532       89         51   \n",
       "LF_abnormal_mesh_terms_in_report         0.138298      141         16   \n",
       "LF_recommend_in_report                   0.002660        6          0   \n",
       "LF_mm_in_report                          0.010638       20          0   \n",
       "LF_normal                                0.114362       34         26   \n",
       "LF_positive_MeshTerm                     0.106383      108          3   \n",
       "LF_fracture                              0.013298       11          1   \n",
       "LF_calcinosis                            0.007979        7          0   \n",
       "LF_degen_spine                           0.015957       42          0   \n",
       "LF_lung_hypoinflation                    0.010638       26          0   \n",
       "LF_lung_hyperdistention                  0.031915       32          3   \n",
       "LF_catheters                             0.007979       12          1   \n",
       "LF_surgical                              0.005319       11          0   \n",
       "LF_granuloma                             0.061170       53          0   \n",
       "\n",
       "                                        Emp. Acc.  \n",
       "LF_report_is_short                       0.631579  \n",
       "LF_consistency_in_report                 0.811170  \n",
       "LF_negative_inflection_words_in_report   0.967742  \n",
       "LF_is_seen_or_noted_in_report            0.781250  \n",
       "LF_disease_in_report                     0.635714  \n",
       "LF_abnormal_mesh_terms_in_report         0.898089  \n",
       "LF_recommend_in_report                   1.000000  \n",
       "LF_mm_in_report                          1.000000  \n",
       "LF_normal                                0.566667  \n",
       "LF_positive_MeshTerm                     0.972973  \n",
       "LF_fracture                              0.916667  \n",
       "LF_calcinosis                            1.000000  \n",
       "LF_degen_spine                           1.000000  \n",
       "LF_lung_hypoinflation                    1.000000  \n",
       "LF_lung_hyperdistention                  0.914286  \n",
       "LF_catheters                             0.923077  \n",
       "LF_surgical                              1.000000  \n",
       "LF_granuloma                             1.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.labeling.analysis import lf_summary\n",
    "\n",
    "# Analyzing LF stats\n",
    "lf_summary(Ls[1], Y=Y_dev, lf_names=lf_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all of our labeling functions, while certainly imperfect, are better than random chance.  This fulfills the only theoretical requirement of the cross-modal data programming algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train a Label Model in Snorkel MeTaL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the Snorkel MeTaL model training API to train a `LabelModel` that learns the accuracies of our LFs.  This is the core step that the data programming technique simplifies and formalizes -- by combining our labeling functions based on their accuracies, we can recover a model that outputs reasonable weak labels.\n",
    "\n",
    "We perform a simple random hyperparameter search over learning rate and L2 regularization, using our small labeled development set to choose the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling.model.label_model import LabelModel\n",
    "\n",
    "label_model = LabelModel(cardinality=2, seed=1701, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing O...\n",
      "Estimating \\mu...\n",
      "[1 epochs]: TRAIN:[loss=1.948]\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "label_model.train_model(Ls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from metal.tuners import RandomSearchTuner\n",
    "\n",
    "# # Creating search space\n",
    "# search_space = {\n",
    "#     \"l2\": {\"range\": [0.0001, 0.1], \"scale\": \"log\"},  # linear range\n",
    "#     \"lr\": {\"range\": [0.0001, 0.1], \"scale\": \"log\"},  # log range\n",
    "# }\n",
    "\n",
    "# searcher = RandomSearchTuner(LabelModel, log_dir=\"./run_logs\", log_writer_class=None)\n",
    "\n",
    "# # Training label model\n",
    "\n",
    "\n",
    "# label_model = searcher.search(\n",
    "#     search_space,\n",
    "#     (Ls[1], Ys[1]),\n",
    "#     train_args=[Ls[0]],\n",
    "#     init_args=[],\n",
    "#     init_kwargs={\"k\": 2, \"seed\": 1701},\n",
    "#     train_kwargs={\"n_epochs\": 200},\n",
    "#     max_search=40,\n",
    "#     verbose=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate our best model on the development set as below -- you should recover a model with best accuracy of approximately 85% on the development set -- this `LabelModel`will be applied to the training set to create weak labels, which we can then use to train our image classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCKED\n",
    "\n",
    "# Getting scores\n",
    "# scores = label_model.score(\n",
    "#     (Ls[1], Ys[1]), metric=[\"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this useful?  If we compare to majority vote, we see a couple points of improvement in accuracy.  Note that the degree to which we expect this model to improve over majority vote varies based on the type of dataset involved, as detailed in the 2018 [VLDB Paper](http://www.vldb.org/pvldb/vol11/p269-ratner.pdf) describing the Snorkel system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCKED\n",
    "\n",
    "# from snorkel.labeling.model.baselines import MajorityLabelVoter\n",
    "\n",
    "# Checking if we beat majority vote\n",
    "# mv = MajorityLabelVoter(seed=123)\n",
    "# scores = mv.score((Ls[1], Ys[1]), metric=[\"accuracy\", \"precision\", \"recall\", \"f1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create a Weakly Labeled Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this trained `LabelModel` to create weak labels for each of our train, development, and test splits by applying it to the label matrices, as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_ps = label_model.predict_proba(Ls[0])\n",
    "Y_dev_ps = label_model.predict_proba(Ls[1])\n",
    "Y_test_ps = label_model.predict_proba(Ls[2])\n",
    "Y_ps = [Y_train_ps, Y_dev_ps, Y_test_ps]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the distribution of our weak training labels, and note that they are assigned varying degrees of probability.  An advantage of this labeling approach is that probabilistic labels can be very descriptive -- i.e., if an example has a 60% probability of being abnormal, we train against that 0.6 probability, rather than binarizing to 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# from metal.contrib.visualization.analysis import plot_probabilities_histogram\n",
    "\n",
    "# Looking at probability histogram for training labels\n",
    "# plot_probabilities_histogram(Y_dev_ps[:, 0], title=\"Probablistic Label Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the development set, we can also check that the class balance of our weak labels if we were to naively binarize at the 0.5 cutoff -- we see reasonable behavior here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# from metal.contrib.visualization.analysis import plot_predictions_histogram\n",
    "\n",
    "# Obtaining binarized predictions\n",
    "# Y_dev_p = label_model.predict(Ls[1])\n",
    "# plot_predictions_histogram(Y_dev_p, Ys[1], title=\"Label Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train a Weakly Supervised End Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our weak training labels, we can train a commodity CNN using a simple PyTorch API.  In Snorkel MeTaL, we have written high-level utilities to do this.  The entire process of defining and training the model can be executed in the following two simple cells.\n",
    "\n",
    "First, we define PyTorch `DataLoader` objects to efficiently load our image data, associating each image with the weak label generated from its associated report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace end model\n",
    "\n",
    "# import torch\n",
    "# from torchvision import models\n",
    "# from metal.end_model import EndModel\n",
    "# from metal.logging.tensorboard import TensorBoardWriter\n",
    "# from utils import get_data_loader\n",
    "\n",
    "# # Setting up log directory\n",
    "# log_config = {\"log_dir\": \"./run_logs\", \"run_name\": \"openi_demo_ws\"}\n",
    "# tuner_config = {\"max_search\": 1}\n",
    "# search_space = {\"l2\": [0.0005], \"lr\": [0.001]}  # linear range\n",
    "\n",
    "# # Create pytorch model\n",
    "# num_classes = 2\n",
    "# cnn_model = models.resnet18(pretrained=True)\n",
    "# last_layer_input_size = int(cnn_model.fc.weight.size()[1])\n",
    "# cnn_model.fc = torch.nn.Linear(last_layer_input_size, num_classes)\n",
    "\n",
    "# # Create data loaders\n",
    "# loaders = {}\n",
    "# loaders[\"train\"] = get_data_loader(\n",
    "#     data[\"train\"][\"xray_paths\"].tolist(), Y_ps[0], batch_size=32, shuffle=True\n",
    "# )\n",
    "# loaders[\"dev\"] = get_data_loader(\n",
    "#     data[\"dev\"][\"xray_paths\"].tolist(), Ys[1], batch_size=32, shuffle=False\n",
    "# )\n",
    "# loaders[\"test\"] = get_data_loader(\n",
    "#     data[\"test\"][\"xray_paths\"].tolist(), Ys[2], batch_size=32, shuffle=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, a single datapoint yields an image like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# img, label = loaders[\"train\"].dataset[0]\n",
    "\n",
    "# plt.figure()\n",
    "# plt.imshow(img[0, :, :], cmap=\"gray\")\n",
    "# plt.title(\"Example X-ray Image\")\n",
    "# ax = plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our `DataLoaders` are set up, it is a simple matter to define and train our CNN model.\n",
    "\n",
    "Note: While this will run if you do not have a CUDA-based GPU available (and will automatically detect it if you do), it will proceed *much* faster if you have one!  CPU-only per-epoch training time is ~ 15 minutes, while with a Titan X it is approximately 30 s!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Defining network parameters\n",
    "# num_classes = 2\n",
    "# pretrained = True\n",
    "# train_args = [loaders[\"train\"]]\n",
    "# init_args = [[num_classes]]\n",
    "\n",
    "# # Defining device variable\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# # Initializing input module\n",
    "# input_module = cnn_model\n",
    "# init_kwargs = {\n",
    "#     \"input_module\": input_module,\n",
    "#     \"skip_head\": True,\n",
    "#     \"input_relu\": False,\n",
    "#     \"input_batchnorm\": False,\n",
    "#     \"device\": device,\n",
    "#     \"seed\": 1701,\n",
    "# }\n",
    "# train_kwargs = {\"n_epochs\": 5, \"progress_bar\": True}\n",
    "\n",
    "# # Setting up logger and searcher\n",
    "# searcher = RandomSearchTuner(\n",
    "#     EndModel,\n",
    "#     **log_config,\n",
    "#     log_writer_class=TensorBoardWriter,\n",
    "#     validation_metric=\"accuracy\",\n",
    "#     seed=1701,\n",
    "# )\n",
    "\n",
    "# # Training weakly supervised model\n",
    "# weakly_supervised_model = searcher.search(\n",
    "#     search_space,\n",
    "#     loaders[\"dev\"],\n",
    "#     train_args=train_args,\n",
    "#     init_args=init_args,\n",
    "#     init_kwargs=init_kwargs,\n",
    "#     train_kwargs=train_kwargs,\n",
    "#     max_search=tuner_config[\"max_search\"],\n",
    "#     clean_up=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate this model below, and see that we've learned some useful signal!  Remember that an Area Under the Receiver Operating Characteristic (ROC-AUC) score represents the probability across all possible cutoffs of ranking an abnormal example higher than a normal example.  If we've learned nothing useful, this value would be 0.5.\n",
    "\n",
    "You should expect a value just above 0.70 for this training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluating model\n",
    "# print(f\"Evaluating Weakly Supervised Model\")\n",
    "# scores = weakly_supervised_model.score(loaders[\"test\"], metric=[\"roc-auc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Comparing to a Fully Supervised End Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have ground-truth labels for the entire dataset in this case (the OpenI dataset comes with these labels, which require physicians to label thousands of images!), we can compare how well our weakly supervised model does with the performance we achieve from a fully supervised model.  This is a similar analysis to that performed in our 2019 Nature submission.\n",
    "\n",
    "Executing this requires a simple change to the training dataloader to provide it with ground-truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Updating logging config\n",
    "# log_config = {\"log_dir\": \"./run_logs\", \"run_name\": \"openi_demo_fs\"}\n",
    "\n",
    "\n",
    "# # Creating dataloader with ground truth training labels\n",
    "# loaders[\"full_train\"] = get_data_loader(\n",
    "#     data[\"train\"][\"xray_paths\"].tolist(), Ys[0], batch_size=32, shuffle=True\n",
    "# )\n",
    "# train_args = [loaders[\"full_train\"]]\n",
    "\n",
    "# # Setting up logger and searcher\n",
    "# searcher = RandomSearchTuner(\n",
    "#     EndModel,\n",
    "#     **log_config,\n",
    "#     log_writer_class=TensorBoardWriter,\n",
    "#     validation_metric=\"accuracy\",\n",
    "#     seed=1701,\n",
    "# )\n",
    "\n",
    "# # Training\n",
    "# fully_supervised_model = searcher.search(\n",
    "#     search_space,\n",
    "#     loaders[\"dev\"],\n",
    "#     train_args=train_args,\n",
    "#     init_args=init_args,\n",
    "#     init_kwargs=init_kwargs,\n",
    "#     train_kwargs=train_kwargs,\n",
    "#     max_search=tuner_config[\"max_search\"],\n",
    "#     clean_up=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can evaluate the weakly and fully supervised models, observing that they achieve similar Area Under the Receiver Operating Characteristic (ROC-AUC) scores.  Note that due to the small size of the dataset and that we are not tuning the cutoff for a particular performance score, we report ROC-AUC in this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluating weakly model\n",
    "# print(f\"Evaluating Weakly Supervised Model\")\n",
    "# weakly_supervised_scores = weakly_supervised_model.score(\n",
    "#     loaders[\"test\"], metric=[\"roc-auc\"], print_confusion_matrix=False\n",
    "# )\n",
    "\n",
    "# # Evaluating fully supervised model\n",
    "# print(f\"Evaluating Fully Supervised Model\")\n",
    "# fully_supervised_scores = fully_supervised_model.score(\n",
    "#     loaders[\"test\"], metric=[\"roc-auc\"], print_confusion_matrix=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the models have trained successfully, you should observe that the weakly and fully supervised models both achieve ROC-AUC scores around 0.70.  This indicates that the weak labels we created using our labeling functions over the text have successfully allowed us to train a CNN model that performs very similarly to one trained using ground truth, but *without having to label thousands of images*.\n",
    "\n",
    "Congratulations! You've just trained a deep learning model using cross-modal data programming!  We hope this demo is helpful in your research, and check for updates to Snorkel and Snorkel MeTaL at [snorkel.stanford.edu](snorkel.stanford.edu)!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
