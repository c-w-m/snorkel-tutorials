{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4019ef52",
   "metadata": {},
   "source": [
    "# Classifying Chest X-rays with Cross-Modal Data Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fba6423",
   "metadata": {},
   "source": [
    "This tutorial demonstrates how to use the *cross-modal data programming* technique described in Dunnmon and Ratner, et al. (2019) to build a Convolutional Neural Network (CNN) model with no hand-labeled data that performs similarly to a CNN supervised using several thousand data points labeled by radiologists.  This process is *exactly* equivalent to that followed for the chest radiograph dataset in our 2019 Nature submission.\n",
    "\n",
    "We begin by setting up our environment, importing relevant Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebf0b230",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# Importing pandas for data processing\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f178ce9",
   "metadata": {},
   "source": [
    "## Step 1: Loading and Splitting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbf41bd",
   "metadata": {},
   "source": [
    "First, we set up the data dictionary and load data that we've already split for you into an (approximately) 80% train split, 10% development split, and 10% test split.  Each raw data point contains three fields: a text report, a label (normal or abnormal), and a set of image paths.  The original data, from the OpenI dataset, is maintained by [NIH](https://openi.nlm.nih.gov/faq.php)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8567f26a",
   "metadata": {},
   "source": [
    "**TODO:**\n",
    "1. Move the dataset adjustment and conversion to a preprocessing script\n",
    "2. Do not include labels column in train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "766ee502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2630 train examples: 63.8% Abnormal\n",
      "376 dev examples: 63.0% Abnormal\n",
      "378 test examples: 61.6% Abnormal\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>xray_paths</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>./data/openi/xrays/CXR1817_IM-0529-4004.png</td>\n",
       "      <td>COMPARISON:  INDICATION: Generalized weakness....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>./data/openi/xrays/CXR2477_IM-1006-1001.png</td>\n",
       "      <td>COMPARISON:  INDICATION: XXXX-year-old female ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>./data/openi/xrays/CXR1969_IM-0630-1001.png</td>\n",
       "      <td>COMPARISON:  INDICATION: kidney cancer f/u. FI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>./data/openi/xrays/CXR2143_IM-0765-1001.png</td>\n",
       "      <td>COMPARISON: XXXX, XXXX INDICATION: XXXX, XXXX ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>./data/openi/xrays/CXR2336_IM-0903-1001.png</td>\n",
       "      <td>COMPARISON: None INDICATION: XXXX-year-old mal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                   xray_paths  \\\n",
       "0      1  ./data/openi/xrays/CXR1817_IM-0529-4004.png   \n",
       "1      1  ./data/openi/xrays/CXR2477_IM-1006-1001.png   \n",
       "2      2  ./data/openi/xrays/CXR1969_IM-0630-1001.png   \n",
       "3      1  ./data/openi/xrays/CXR2143_IM-0765-1001.png   \n",
       "4      1  ./data/openi/xrays/CXR2336_IM-0903-1001.png   \n",
       "\n",
       "                                                text  \n",
       "0  COMPARISON:  INDICATION: Generalized weakness....  \n",
       "1  COMPARISON:  INDICATION: XXXX-year-old female ...  \n",
       "2  COMPARISON:  INDICATION: kidney cancer f/u. FI...  \n",
       "3  COMPARISON: XXXX, XXXX INDICATION: XXXX, XXXX ...  \n",
       "4  COMPARISON: None INDICATION: XXXX-year-old mal...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up data dictionary and defining data splits\n",
    "data = {}\n",
    "splits = [\"train\", \"dev\", \"test\"]\n",
    "\n",
    "for split in splits:\n",
    "    data[split] = pd.read_csv(f\"data/{split}_entries.csv\")[\n",
    "        [\"label\", \"xray_paths\", \"text\"]\n",
    "    ]\n",
    "    # Adjusting labels to fit with Snorkel MeTaL labeling convention\n",
    "    data[split][\"label\"][data[split][\"label\"] == 0] = 2\n",
    "    perc_pos = sum(data[split][\"label\"] == 1) / len(data[split])\n",
    "    print(f\"{len(data[split])} {split} examples: {100*perc_pos:0.1f}% Abnormal\")\n",
    "    \n",
    "train_df = pd.DataFrame.from_dict(data[\"train\"])\n",
    "valid_df = pd.DataFrame.from_dict(data[\"dev\"])\n",
    "test_df = pd.DataFrame.from_dict(data[\"test\"])\n",
    "\n",
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6648168",
   "metadata": {},
   "source": [
    "You can see an example of a single data point below -- note that the raw label convention for our normal vs. abnormal classification problem is 1 for abnormal and 2 for normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99725b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW TEXT:\n",
      " \n",
      " COMPARISON:  INDICATION: Generalized weakness. FINDINGS: The XXXX examination consists of frontal and lateral radiographs of the chest. Upper thorax is poorly visualized due to patient's overlying head and chin. The cardiomediastinal contours are within normal limits. Background of mild coarse interstitial opacities seen throughout the lungs XXXX related to background of emphysema. Calcified granuloma is seen in the left medial lung base. There is no consolidation, pleural effusion or pneumothorax. Deformity of the right 6th rib laterally has appearance of acute or subacute fracture. Degenerative changes of the thoracic spine are again seen. IMPRESSION: Age indeterminant but XXXX acute to subacute right 6th rib fracture. \n",
      "\n",
      "IMAGE PATHS: \n",
      " \n",
      " ./data/openi/xrays/CXR1817_IM-0529-4004.png \n",
      "\n",
      "LABEL: 1\n"
     ]
    }
   ],
   "source": [
    "sample = valid_df.iloc[0]\n",
    "print(\"RAW TEXT:\\n \\n\", sample[\"text\"], \"\\n\")\n",
    "print(\"IMAGE PATHS: \\n \\n\", sample[\"xray_paths\"], \"\\n\")\n",
    "print(\"LABEL:\", sample[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64a09d9",
   "metadata": {},
   "source": [
    "## Step 2: Developing LFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078b0789",
   "metadata": {},
   "source": [
    "We now define our *labeling functions* (LFs): simple, heuristic functions written by a domain expert (e.g., a radiologist) that correctly label a report as normal or abnormal with probability better than random chance.\n",
    "\n",
    "We give an example of all three types of LFs we reference in the paper: general pattern LFs that operate on patterns a non-expert user could easily identify, medical pattern LFs that operate on patterns easily identifiable by a clinician, and structural LFs that focus on specific structural elements of the report (e.g. how long it is) that have some correlation with the scan it describes being normal or abnormal.\n",
    "\n",
    "**TODO:**\n",
    "1. Convert regex things to preprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f38543fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PandasLFApplier'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d276b9d7498c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msnorkel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabeling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPandasLFApplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msnorkel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabeling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlabeling_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Value to use for abstain votes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'PandasLFApplier'"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling.apply import PandasLFApplier\n",
    "from snorkel.labeling.lf import labeling_function\n",
    "import re\n",
    "\n",
    "# Value to use for abstain votes\n",
    "ABSTAIN = 0\n",
    "# Value to use for abnormal votes\n",
    "ABNORMAL = 1\n",
    "# Value to user for normal votes\n",
    "NORMAL = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a445c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs = []\n",
    "\n",
    "# Example of a General Pattern LF\n",
    "noted_or_seen = [\"is seen\", \"noted\"]\n",
    "@labeling_function()\n",
    "def LF_noted_or_seen(x, noted_or_seen):\n",
    "    if any(word in x.text.lower() for word in noted_or_seen):\n",
    "        return ABNORMAL\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "lfs.append(LF_noted_or_seen)\n",
    "\n",
    "@labeling_function()\n",
    "def LF_negative(x, negative_words):\n",
    "    return (\n",
    "        ABNORMAL\n",
    "        if any(word in x.text.lower() for word in  [\"but\", \"however\", \"otherwise\"])\n",
    "        else ABSTAIN\n",
    "    )\n",
    "lfs.append(LF_negative)\n",
    "\n",
    "@labeling_function()\n",
    "def LF_disease_in_report(x):\n",
    "    return ABNORMAL if \"disease\" in x.text.lower() else ABSTAIN\n",
    "lfs.append(LF_disease_in_report)\n",
    "\n",
    "@labeling_function()\n",
    "def LF_recommend_in_report(x):\n",
    "    return ABNORMAL if \"recommend\" in x.text.lower() else ABSTAIN\n",
    "lfs.append(LF_recommend_in_report)\n",
    "\n",
    "@labeling_function()\n",
    "def LF_mm_in_report(x):\n",
    "    return ABNORMAL if any(word in x.text.lower() for word in [\"mm\", \"cm\"]) else ABSTAIN\n",
    "lfs.append(LF_mm_in_report)\n",
    "\n",
    "# Example of a Medical Pattern LF\n",
    "@labeling_function()\n",
    "def LF_lung_hyperdistention_demo(x):\n",
    "    \"\"\"\n",
    "    Votes abnormal for indications of lung hyperdistention.\n",
    "    \"\"\"\n",
    "    reg_01 = re.compile(\"increased volume|hyperexpan|inflated\", re.IGNORECASE)\n",
    "    for s in x.text.split(\".\"):\n",
    "        if reg_01.search(s):\n",
    "            return ABNORMAL\n",
    "    return ABSTAIN\n",
    "lfs.append(LF_lung_hyperdistention_demo)\n",
    "\n",
    "@labeling_function()\n",
    "def LF_consistency_in_report(x):\n",
    "    \"\"\"\n",
    "    The words 'clear', 'no', 'normal', 'free', 'midline' in\n",
    "    findings section of the report\n",
    "    \"\"\"\n",
    "    findings = x.text[x.text.find(\"FINDINGS:\") :]\n",
    "    findings = findings[: findings.find(\"IMPRESSION:\")]\n",
    "    sents = findings.split(\".\")\n",
    "\n",
    "    normalcy_words = [\"clear\", \"no\", \"normal\", \"unremarkable\", \"free\", \"midline\"]\n",
    "    num_sents_without_normal = ABSTAIN\n",
    "    for sent in sents:\n",
    "        sent = sent.lower()\n",
    "        if not any(word in sent for word in normalcy_words):\n",
    "            num_sents_without_normal += 1\n",
    "        elif \"not\" in sent:\n",
    "            num_sents_without_normal += 1\n",
    "    return NORMAL if num_sents_without_normal < 2 else ABNORMAL\n",
    "lfs.append(LF_consistency_in_report)\n",
    "\n",
    "abnormal_mesh_terms = [\n",
    "    \"opacity\",\n",
    "    \"cardiomegaly\",\n",
    "    \"calcinosis\",\n",
    "    \"hypoinflation\",\n",
    "    \"calcified granuloma\",\n",
    "    \"thoracic vertebrae\",\n",
    "    \"degenerative\",\n",
    "    \"hyperdistention\",\n",
    "    \"catheters\",\n",
    "    \"granulomatous\",\n",
    "    \"nodule\",\n",
    "    \"fracture\" \"surgical\",\n",
    "    \"instruments\",\n",
    "    \"emphysema\",\n",
    "]\n",
    "@labeling_function(resources=dict(abnormal_mesh_terms=abnormal_mesh_terms))\n",
    "def LF_abnormal_mesh_terms_in_report(x, abnormal_mesh_terms):\n",
    "    if any(mesh in x.text.lower() for mesh in abnormal_mesh_terms):\n",
    "        return ABNORMAL\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "lfs.append(LF_abnormal_mesh_terms_in_report)\n",
    "\n",
    "# Example of a Structural LF\n",
    "@labeling_function()\n",
    "def LF_report_is_short_demo(x):\n",
    "    \"\"\"\n",
    "    Checks if report is short.\n",
    "    \"\"\"\n",
    "    return NORMAL if len(x.text) < 280 else ABSTAIN\n",
    "lfs.append(LF_report_is_short_demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a892fe5",
   "metadata": {},
   "source": [
    "Now, we can see how well these LFs might do at correctly indicating normal or abnormal examples by first applying the labeling functions to the examples and then printing some useful statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7684c48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling.apply import PandasLFApplier\n",
    "\n",
    "applier = PandasLFApplier(lfs)\n",
    "L_train = applier.apply(train_df)\n",
    "L_valid = applier.apply(valid_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecf057d",
   "metadata": {},
   "source": [
    "If we use analyze the `LF_lung_hyperdistention_demo` function -- in this case,  we see that it has polarity [1,2], meaning it votes on both class 1 and class 2 (and votes on every example because `coverage` = 1.0), but that it has low accuracy (around 44%).  Let's look at the confusion matrix to see why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c646f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.analysis.utils import convert_labels\n",
    "from snorkel.labeling.analysis import lf_summary\n",
    "\n",
    "Y_valid = valid_df.label.values\n",
    "lf_names= [lf.name for lf in lfs]\n",
    "lf_summary(L_valid, Y_valid, lf_names=lf_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1c5830",
   "metadata": {},
   "source": [
    "**TODO: don't have interactive components that require users to change for good result in the tutorial**\n",
    "\n",
    "Clearly, this LF is much more accurate on abnormal examples (where y=1) than on abnormal examples (where y=2).  Why don't we adjust it to only vote in the positive direction and see how we do?\n",
    "\n",
    "Go ahead and change `NORMAL` to `ABSTAIN` in the `LF_lung_hyperdistention_demo` function (the line below the `### *** ###` comment), and rerun the last three code cells.\n",
    "\n",
    "You'll see that by making this rule a bit more targeted, its coverage decreases to 9%, but it's accuracy jumps to over 90%.  This type of iteration is exactly how clinicians can develop LFs in practice.\n",
    "\n",
    "You may also notice that it's very easy to write these LFs over text, but it would be very hard to, say, write an `LF_lung_hyperdistention` version that operates over an image -- this is why cross-modality is so important!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbc0266",
   "metadata": {},
   "source": [
    "Once we've designed a couple of LFs, it's time to execute them all on every example we have to create a *label matrix*.  This is an $n$ by $m$ matrix, where $n$ is the number of examples and $m$ is the number of LFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89311a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from labeling_functions import (\n",
    "#     LF_report_is_short,\n",
    "#     LF_consistency_in_report,\n",
    "#     LF_negative_inflection_words_in_report,\n",
    "#     LF_is_seen_or_noted_in_report,\n",
    "#     LF_disease_in_report,\n",
    "#     LF_abnormal_mesh_terms_in_report,\n",
    "#     LF_recommend_in_report,\n",
    "#     LF_mm_in_report,\n",
    "#     LF_normal,\n",
    "#     LF_positive_MeshTerm,\n",
    "#     LF_fracture,\n",
    "#     LF_calcinosis,\n",
    "#     LF_degen_spine,\n",
    "#     LF_lung_hypoinflation,\n",
    "#     LF_lung_hyperdistention,\n",
    "#     LF_catheters,\n",
    "#     LF_surgical,\n",
    "#     LF_granuloma,\n",
    "# )\n",
    "\n",
    "# lfs = [\n",
    "#     LF_report_is_short,\n",
    "#     LF_consistency_in_report,\n",
    "#     LF_negative_inflection_words_in_report,\n",
    "#     LF_is_seen_or_noted_in_report,\n",
    "#     LF_disease_in_report,\n",
    "#     LF_abnormal_mesh_terms_in_report,\n",
    "#     LF_recommend_in_report,\n",
    "#     LF_mm_in_report,\n",
    "#     LF_normal,\n",
    "#     LF_positive_MeshTerm,\n",
    "#     LF_fracture,\n",
    "#     LF_calcinosis,\n",
    "#     LF_degen_spine,\n",
    "#     LF_lung_hypoinflation,\n",
    "#     LF_lung_hyperdistention,\n",
    "#     LF_catheters,\n",
    "#     LF_surgical,\n",
    "#     LF_granuloma,\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b39f932",
   "metadata": {},
   "source": [
    "## Step 4: Train a Label Model in Snorkel MeTaL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f121f1",
   "metadata": {},
   "source": [
    "Next, we use the Snorkel MeTaL model training API to train a `LabelModel` that learns the accuracies of our LFs.  This is the core step that the data programming technique simplifies and formalizes -- by combining our labeling functions based on their accuracies, we can recover a model that outputs reasonable weak labels.\n",
    "\n",
    "We perform a simple random hyperparameter search over learning rate and L2 regularization, using our small labeled development set to choose the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17cf4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling.model import LabelModel\n",
    "from snorkel.analysis.utils import probs_to_preds\n",
    "from snorkel.analysis.metrics import metric_score\n",
    "\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.train_model(L_train, log_train_every=10, lr=0.05, class_balance=[0.7, 0.3], n_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afba3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_probs_valid = label_model.predict_proba(L_valid)\n",
    "Y_preds_valid = probs_to_preds(Y_probs_valid)\n",
    "metric_score(Y_valid, Y_preds_valid, probs=None, metric=\"f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7909fa",
   "metadata": {},
   "source": [
    "**Majority Vote**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c44617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling.model import MajorityLabelVoter\n",
    "\n",
    "mv_model = MajorityLabelVoter()\n",
    "Y_probs_valid = mv_model.predict_proba(L_valid)\n",
    "Y_preds_valid = probs_to_preds(Y_probs_valid)\n",
    "metric_score(Y_valid, Y_preds_valid, probs=None, metric=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab56fdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from metal.tuners import RandomSearchTuner\n",
    "\n",
    "# # Creating search space\n",
    "# search_space = {\n",
    "#     \"l2\": {\"range\": [0.0001, 0.1], \"scale\": \"log\"},  # linear range\n",
    "#     \"lr\": {\"range\": [0.0001, 0.1], \"scale\": \"log\"},  # log range\n",
    "# }\n",
    "\n",
    "# searcher = RandomSearchTuner(LabelModel, log_dir=\"./run_logs\", log_writer_class=None)\n",
    "\n",
    "# # Training label model\n",
    "\n",
    "\n",
    "# label_model = searcher.search(\n",
    "#     search_space,\n",
    "#     (Ls[1], Ys[1]),\n",
    "#     train_args=[Ls[0]],\n",
    "#     init_args=[],\n",
    "#     init_kwargs={\"k\": 2, \"seed\": 1701},\n",
    "#     train_kwargs={\"n_epochs\": 200},\n",
    "#     max_search=40,\n",
    "#     verbose=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0f671e",
   "metadata": {},
   "source": [
    "We evaluate our best model on the development set as below -- you should recover a model with best accuracy of approximately 85% on the development set -- this `LabelModel`will be applied to the training set to create weak labels, which we can then use to train our image classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "caf677c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCKED\n",
    "\n",
    "# Getting scores\n",
    "# scores = label_model.score(\n",
    "#     (Ls[1], Ys[1]), metric=[\"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e1c602",
   "metadata": {},
   "source": [
    "Why is this useful?  If we compare to majority vote, we see a couple points of improvement in accuracy.  Note that the degree to which we expect this model to improve over majority vote varies based on the type of dataset involved, as detailed in the 2018 [VLDB Paper](http://www.vldb.org/pvldb/vol11/p269-ratner.pdf) describing the Snorkel system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed798cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCKED\n",
    "\n",
    "# from snorkel.labeling.model.baselines import MajorityLabelVoter\n",
    "\n",
    "# Checking if we beat majority vote\n",
    "# mv = MajorityLabelVoter(seed=123)\n",
    "# scores = mv.score((Ls[1], Ys[1]), metric=[\"accuracy\", \"precision\", \"recall\", \"f1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254e4bad",
   "metadata": {},
   "source": [
    "## Step 5: Create a Weakly Labeled Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239b701f",
   "metadata": {},
   "source": [
    "We can now use this trained `LabelModel` to create weak labels for each of our train, development, and test splits by applying it to the label matrices, as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5de4f563",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_ps = label_model.predict_proba(Ls[0])\n",
    "Y_dev_ps = label_model.predict_proba(Ls[1])\n",
    "Y_test_ps = label_model.predict_proba(Ls[2])\n",
    "Y_ps = [Y_train_ps, Y_dev_ps, Y_test_ps]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c75f2ee",
   "metadata": {},
   "source": [
    "We can inspect the distribution of our weak training labels, and note that they are assigned varying degrees of probability.  An advantage of this labeling approach is that probabilistic labels can be very descriptive -- i.e., if an example has a 60% probability of being abnormal, we train against that 0.6 probability, rather than binarizing to 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b63179a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# from metal.contrib.visualization.analysis import plot_probabilities_histogram\n",
    "\n",
    "# Looking at probability histogram for training labels\n",
    "# plot_probabilities_histogram(Y_dev_ps[:, 0], title=\"Probablistic Label Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cf0739",
   "metadata": {},
   "source": [
    "Using the development set, we can also check that the class balance of our weak labels if we were to naively binarize at the 0.5 cutoff -- we see reasonable behavior here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f07ba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# from metal.contrib.visualization.analysis import plot_predictions_histogram\n",
    "\n",
    "# Obtaining binarized predictions\n",
    "# Y_dev_p = label_model.predict(Ls[1])\n",
    "# plot_predictions_histogram(Y_dev_p, Ys[1], title=\"Label Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c2b3d9",
   "metadata": {},
   "source": [
    "## Step 6: Train a Weakly Supervised End Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8535579",
   "metadata": {},
   "source": [
    "Now that we have our weak training labels, we can train a commodity CNN using a simple PyTorch API.  In Snorkel MeTaL, we have written high-level utilities to do this.  The entire process of defining and training the model can be executed in the following two simple cells.\n",
    "\n",
    "First, we define PyTorch `DataLoader` objects to efficiently load our image data, associating each image with the weak label generated from its associated report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b205e352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace end model\n",
    "\n",
    "# import torch\n",
    "# from torchvision import models\n",
    "# from metal.end_model import EndModel\n",
    "# from metal.logging.tensorboard import TensorBoardWriter\n",
    "# from utils import get_data_loader\n",
    "\n",
    "# # Setting up log directory\n",
    "# log_config = {\"log_dir\": \"./run_logs\", \"run_name\": \"openi_demo_ws\"}\n",
    "# tuner_config = {\"max_search\": 1}\n",
    "# search_space = {\"l2\": [0.0005], \"lr\": [0.001]}  # linear range\n",
    "\n",
    "# # Create pytorch model\n",
    "# num_classes = 2\n",
    "# cnn_model = models.resnet18(pretrained=True)\n",
    "# last_layer_input_size = int(cnn_model.fc.weight.size()[1])\n",
    "# cnn_model.fc = torch.nn.Linear(last_layer_input_size, num_classes)\n",
    "\n",
    "# # Create data loaders\n",
    "# loaders = {}\n",
    "# loaders[\"train\"] = get_data_loader(\n",
    "#     data[\"train\"][\"xray_paths\"].tolist(), Y_ps[0], batch_size=32, shuffle=True\n",
    "# )\n",
    "# loaders[\"dev\"] = get_data_loader(\n",
    "#     data[\"dev\"][\"xray_paths\"].tolist(), Ys[1], batch_size=32, shuffle=False\n",
    "# )\n",
    "# loaders[\"test\"] = get_data_loader(\n",
    "#     data[\"test\"][\"xray_paths\"].tolist(), Ys[2], batch_size=32, shuffle=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb9c3ea",
   "metadata": {},
   "source": [
    "As an example, a single datapoint yields an image like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65e99c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# img, label = loaders[\"train\"].dataset[0]\n",
    "\n",
    "# plt.figure()\n",
    "# plt.imshow(img[0, :, :], cmap=\"gray\")\n",
    "# plt.title(\"Example X-ray Image\")\n",
    "# ax = plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caa5f5a",
   "metadata": {},
   "source": [
    "Now that our `DataLoaders` are set up, it is a simple matter to define and train our CNN model.\n",
    "\n",
    "Note: While this will run if you do not have a CUDA-based GPU available (and will automatically detect it if you do), it will proceed *much* faster if you have one!  CPU-only per-epoch training time is ~ 15 minutes, while with a Titan X it is approximately 30 s!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b24f1b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Defining network parameters\n",
    "# num_classes = 2\n",
    "# pretrained = True\n",
    "# train_args = [loaders[\"train\"]]\n",
    "# init_args = [[num_classes]]\n",
    "\n",
    "# # Defining device variable\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# # Initializing input module\n",
    "# input_module = cnn_model\n",
    "# init_kwargs = {\n",
    "#     \"input_module\": input_module,\n",
    "#     \"skip_head\": True,\n",
    "#     \"input_relu\": False,\n",
    "#     \"input_batchnorm\": False,\n",
    "#     \"device\": device,\n",
    "#     \"seed\": 1701,\n",
    "# }\n",
    "# train_kwargs = {\"n_epochs\": 5, \"progress_bar\": True}\n",
    "\n",
    "# # Setting up logger and searcher\n",
    "# searcher = RandomSearchTuner(\n",
    "#     EndModel,\n",
    "#     **log_config,\n",
    "#     log_writer_class=TensorBoardWriter,\n",
    "#     validation_metric=\"accuracy\",\n",
    "#     seed=1701,\n",
    "# )\n",
    "\n",
    "# # Training weakly supervised model\n",
    "# weakly_supervised_model = searcher.search(\n",
    "#     search_space,\n",
    "#     loaders[\"dev\"],\n",
    "#     train_args=train_args,\n",
    "#     init_args=init_args,\n",
    "#     init_kwargs=init_kwargs,\n",
    "#     train_kwargs=train_kwargs,\n",
    "#     max_search=tuner_config[\"max_search\"],\n",
    "#     clean_up=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eada35a8",
   "metadata": {},
   "source": [
    "We can evaluate this model below, and see that we've learned some useful signal!  Remember that an Area Under the Receiver Operating Characteristic (ROC-AUC) score represents the probability across all possible cutoffs of ranking an abnormal example higher than a normal example.  If we've learned nothing useful, this value would be 0.5.\n",
    "\n",
    "You should expect a value just above 0.70 for this training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1c3fd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluating model\n",
    "# print(f\"Evaluating Weakly Supervised Model\")\n",
    "# scores = weakly_supervised_model.score(loaders[\"test\"], metric=[\"roc-auc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc32fe1",
   "metadata": {},
   "source": [
    "## Step 7: Comparing to a Fully Supervised End Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ecd64b",
   "metadata": {},
   "source": [
    "Because we have ground-truth labels for the entire dataset in this case (the OpenI dataset comes with these labels, which require physicians to label thousands of images!), we can compare how well our weakly supervised model does with the performance we achieve from a fully supervised model.  This is a similar analysis to that performed in our 2019 Nature submission.\n",
    "\n",
    "Executing this requires a simple change to the training dataloader to provide it with ground-truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "812cf574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Updating logging config\n",
    "# log_config = {\"log_dir\": \"./run_logs\", \"run_name\": \"openi_demo_fs\"}\n",
    "\n",
    "\n",
    "# # Creating dataloader with ground truth training labels\n",
    "# loaders[\"full_train\"] = get_data_loader(\n",
    "#     data[\"train\"][\"xray_paths\"].tolist(), Ys[0], batch_size=32, shuffle=True\n",
    "# )\n",
    "# train_args = [loaders[\"full_train\"]]\n",
    "\n",
    "# # Setting up logger and searcher\n",
    "# searcher = RandomSearchTuner(\n",
    "#     EndModel,\n",
    "#     **log_config,\n",
    "#     log_writer_class=TensorBoardWriter,\n",
    "#     validation_metric=\"accuracy\",\n",
    "#     seed=1701,\n",
    "# )\n",
    "\n",
    "# # Training\n",
    "# fully_supervised_model = searcher.search(\n",
    "#     search_space,\n",
    "#     loaders[\"dev\"],\n",
    "#     train_args=train_args,\n",
    "#     init_args=init_args,\n",
    "#     init_kwargs=init_kwargs,\n",
    "#     train_kwargs=train_kwargs,\n",
    "#     max_search=tuner_config[\"max_search\"],\n",
    "#     clean_up=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9891fe1",
   "metadata": {},
   "source": [
    "Now, we can evaluate the weakly and fully supervised models, observing that they achieve similar Area Under the Receiver Operating Characteristic (ROC-AUC) scores.  Note that due to the small size of the dataset and that we are not tuning the cutoff for a particular performance score, we report ROC-AUC in this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f92a0dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluating weakly model\n",
    "# print(f\"Evaluating Weakly Supervised Model\")\n",
    "# weakly_supervised_scores = weakly_supervised_model.score(\n",
    "#     loaders[\"test\"], metric=[\"roc-auc\"], print_confusion_matrix=False\n",
    "# )\n",
    "\n",
    "# # Evaluating fully supervised model\n",
    "# print(f\"Evaluating Fully Supervised Model\")\n",
    "# fully_supervised_scores = fully_supervised_model.score(\n",
    "#     loaders[\"test\"], metric=[\"roc-auc\"], print_confusion_matrix=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb51f313",
   "metadata": {},
   "source": [
    "If the models have trained successfully, you should observe that the weakly and fully supervised models both achieve ROC-AUC scores around 0.70.  This indicates that the weak labels we created using our labeling functions over the text have successfully allowed us to train a CNN model that performs very similarly to one trained using ground truth, but *without having to label thousands of images*.\n",
    "\n",
    "Congratulations! You've just trained a deep learning model using cross-modal data programming!  We hope this demo is helpful in your research, and check for updates to Snorkel and Snorkel MeTaL at [snorkel.stanford.edu](snorkel.stanford.edu)!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "snorkel37",
   "language": "python",
   "name": "snorkel37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
